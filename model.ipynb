{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac0f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793db103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = idx2numpy.convert_from_file('data/train.idx3-ubyte')\n",
    "labels = idx2numpy.convert_from_file('data/labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd22d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,W,H = data.shape[0], data.shape[1], data.shape[2]\n",
    "data = data / data.reshape(B * W * H).max().item()\n",
    "data = data.reshape(B, W* H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4488eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_split(data,label,train,dev):\n",
    "    Ntr = int(data.shape[0] * train)\n",
    "    Ndev = int(data.shape[0] * dev)\n",
    "    ind = np.random.randint(low=0,high=data.shape[0],size=(data.shape[0],))\n",
    "    data_shuffled = data[ind]\n",
    "    label_shuffled = label[ind]\n",
    "    Xtr = data_shuffled[:Ntr]\n",
    "    Ytr = label_shuffled[:Ntr]\n",
    "    Xdev = data_shuffled[Ntr:Ntr+Ndev]\n",
    "    Ydev = label_shuffled[Ntr:Ntr+Ndev]\n",
    "    return Xtr,Ytr,Xdev,Ydev\n",
    "Xtr,Ytr,Xdev,Ydev = train_dev_split(data,labels, 0.9,0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8f5a4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_neurons_layer1 = 100\n",
    "number_neurons_layer2 = 100\n",
    "number_neurons_layer3 = 10\n",
    "number_inputs = W*H\n",
    "limit1 = np.sqrt(6/(number_inputs + number_neurons_layer1))\n",
    "W1 = np.random.uniform(-limit1, limit1, size=(number_inputs,number_neurons_layer1))\n",
    "b1 = np.random.randn(number_neurons_layer1) * 0\n",
    "limit2 = np.sqrt(6 / (number_neurons_layer1 + number_neurons_layer2))\n",
    "W2 = np.random.uniform(-limit2,limit2, size=(number_neurons_layer1,number_neurons_layer2))\n",
    "b2 = np.random.randn(number_neurons_layer2) * 0\n",
    "limit3 = np.sqrt(6 / (number_neurons_layer2 + number_neurons_layer3))\n",
    "W3 = np.random.uniform(-limit3,limit3, size=(number_neurons_layer2,number_neurons_layer3))\n",
    "b3 = np.random.randn(number_neurons_layer3) * 0\n",
    "parameters = [W1,b1,W2,b2,W3,b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "e14a1ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch mean: 0.13374169980492195 std: 0.310971277240233\n",
      "Layer 1 mean: -0.0030163679184008734 std: 0.4293240829955881\n",
      "Layer 2 mean: 0.01593556605080713 std: 0.3702864400635133\n",
      "Layer 3 mean: -0.1700615625251002 std: 0.5146610162819465\n",
      "Loss = 2.4814417206961457\n",
      "Loss = 2.4035057631897305\n",
      "Loss = 2.4169340611114873\n",
      "Loss = 2.520697950125048\n",
      "Loss = 2.377714461737032\n",
      "Loss = 2.417370707726943\n",
      "Loss = 2.4446521576450486\n",
      "Loss = 2.4213434684113913\n",
      "Loss = 2.3340700456473864\n",
      "Loss = 2.3726148224268293\n",
      "Loss = 2.3393229658312933\n",
      "Loss = 2.361924682467492\n",
      "Loss = 2.403560246142213\n",
      "Loss = 2.2846209015206487\n",
      "Loss = 2.329555257248063\n",
      "Loss = 2.347235192254936\n",
      "Loss = 2.355646963774813\n",
      "Loss = 2.3486701524819273\n",
      "Loss = 2.3405225450445806\n",
      "Loss = 2.340950413511094\n",
      "Loss = 2.3872900023490873\n",
      "Loss = 2.3572338775262454\n",
      "Loss = 2.339902603554858\n",
      "Loss = 2.274477187410847\n",
      "Loss = 2.311858238273072\n",
      "Loss = 2.366262323606522\n",
      "Loss = 2.3369364401181585\n",
      "Loss = 2.323490939245259\n",
      "Loss = 2.345984062905555\n",
      "Loss = 2.328937205754888\n",
      "Loss = 2.3252150193452734\n",
      "Loss = 2.3912995360599165\n",
      "Loss = 2.315648428645392\n",
      "Loss = 2.3597615968586814\n",
      "Loss = 2.419383335365562\n",
      "Loss = 2.3376926051170557\n",
      "Loss = 2.375140638500681\n",
      "Loss = 2.331914927712182\n",
      "Loss = 2.3480908536878533\n",
      "Loss = 2.336862030081223\n",
      "Loss = 2.3686812859358914\n",
      "Loss = 2.3032994503076987\n",
      "Loss = 2.3577839009827954\n",
      "Loss = 2.3528458757838995\n",
      "Loss = 2.309512116669998\n",
      "Loss = 2.3329157398094233\n",
      "Loss = 2.307823656183407\n",
      "Loss = 2.3708546951024547\n",
      "Loss = 2.367338623381952\n",
      "Loss = 2.3507773650844035\n",
      "Loss = 2.333964479701086\n",
      "Loss = 2.384429093999474\n",
      "Loss = 2.3732661054551496\n",
      "Loss = 2.334851207669483\n",
      "Loss = 2.367407988607784\n",
      "Loss = 2.3580199881989827\n",
      "Loss = 2.301360677318672\n",
      "Loss = 2.334834013041268\n",
      "Loss = 2.3669650456539895\n",
      "Loss = 2.298814009739559\n",
      "Loss = 2.36091318176214\n",
      "Loss = 2.322880953508335\n",
      "Loss = 2.3768802192131115\n",
      "Loss = 2.3077998069458436\n",
      "Loss = 2.323255642503571\n",
      "Loss = 2.3607942686726413\n",
      "Loss = 2.354005746005372\n",
      "Loss = 2.330519830905942\n",
      "Loss = 2.297644112467701\n",
      "Loss = 2.3509051013517546\n",
      "Loss = 2.348249374499317\n",
      "Loss = 2.322439677426745\n",
      "Loss = 2.3451577431627424\n",
      "Loss = 2.3542426069537976\n",
      "Loss = 2.3533534071680813\n",
      "Loss = 2.3400919306725676\n",
      "Loss = 2.3696578535897843\n",
      "Loss = 2.3631773876951936\n",
      "Loss = 2.3179021558546102\n",
      "Loss = 2.3451816359997504\n",
      "Loss = 2.305998871103559\n",
      "Loss = 2.3274262934350434\n",
      "Loss = 2.310495558262681\n",
      "Loss = 2.293530723732304\n",
      "Loss = 2.332740203972756\n",
      "Loss = 2.31111048289722\n",
      "Loss = 2.308373935170986\n",
      "Loss = 2.3208432957382907\n",
      "Loss = 2.3455584353703354\n",
      "Loss = 2.324266348125054\n",
      "Loss = 2.3297513919977213\n",
      "Loss = 2.33810220950824\n",
      "Loss = 2.323936243867692\n",
      "Loss = 2.3246743533503356\n",
      "Loss = 2.355095708587861\n",
      "Loss = 2.3022735023023406\n",
      "Loss = 2.348121993823539\n",
      "Loss = 2.329151016337444\n",
      "Loss = 2.334324664792996\n",
      "Loss = 2.3298774200196783\n",
      "Layer 1 mean: -0.010253460702755185 std: 0.44143680793030626\n",
      "Layer 2 mean: 0.01239686039913162 std: 0.3730537009882966\n",
      "Layer 3 mean: -0.12275984948073107 std: 0.28637125959201576\n",
      "Loss = 2.329531253652363\n",
      "Loss = 2.3439091499453157\n",
      "Loss = 2.32467656521252\n",
      "Loss = 2.3319608625603827\n",
      "Loss = 2.3464124224180063\n",
      "Loss = 2.293068849238157\n",
      "Loss = 2.3715434755343856\n",
      "Loss = 2.3654702347364758\n",
      "Loss = 2.3384300543842302\n",
      "Loss = 2.36415252445639\n",
      "Loss = 2.2909251749505377\n",
      "Loss = 2.2866120156735708\n",
      "Loss = 2.296578097487192\n",
      "Loss = 2.3585789642561146\n",
      "Loss = 2.3655913452018766\n",
      "Loss = 2.353424456463358\n",
      "Loss = 2.3728319945098777\n",
      "Loss = 2.3345305721438785\n",
      "Loss = 2.3892869341279352\n",
      "Loss = 2.3389118163589897\n",
      "Loss = 2.36919575643421\n",
      "Loss = 2.346610198065612\n",
      "Loss = 2.355746869392392\n",
      "Loss = 2.3283741481627063\n",
      "Loss = 2.340568899916529\n",
      "Loss = 2.3457341072662237\n",
      "Loss = 2.3318218213528255\n",
      "Loss = 2.346875146333124\n",
      "Loss = 2.3065611578798286\n",
      "Loss = 2.362624926976606\n",
      "Loss = 2.308938232258087\n",
      "Loss = 2.3454243022261387\n",
      "Loss = 2.33444214928128\n",
      "Loss = 2.3523530846762704\n",
      "Loss = 2.358086490988014\n",
      "Loss = 2.3418926248705327\n",
      "Loss = 2.3058776122369733\n",
      "Loss = 2.345722618572691\n",
      "Loss = 2.316487028554575\n",
      "Loss = 2.327055247357305\n",
      "Loss = 2.311167247709041\n",
      "Loss = 2.302519384542885\n",
      "Loss = 2.3131664901101634\n",
      "Loss = 2.344474264099719\n",
      "Loss = 2.3402837976109376\n",
      "Loss = 2.301159395687493\n",
      "Loss = 2.3332172212919335\n",
      "Loss = 2.3183331803365426\n",
      "Loss = 2.2813055039287153\n",
      "Loss = 2.353172922130753\n",
      "Loss = 2.321358007509915\n",
      "Loss = 2.2994605478749506\n",
      "Loss = 2.299395569011555\n",
      "Loss = 2.320304524824471\n",
      "Loss = 2.341785152284863\n",
      "Loss = 2.3144963048463456\n",
      "Loss = 2.301043917949218\n",
      "Loss = 2.3158947720118004\n",
      "Loss = 2.3288198033877308\n",
      "Loss = 2.3171652951942745\n",
      "Loss = 2.345880994566592\n",
      "Loss = 2.370493198757961\n",
      "Loss = 2.3022981819677355\n",
      "Loss = 2.360933701531298\n",
      "Loss = 2.35797937979546\n",
      "Loss = 2.3642069931462393\n",
      "Loss = 2.345631449715696\n",
      "Loss = 2.3183381105560565\n",
      "Loss = 2.3151281245320416\n",
      "Loss = 2.332633910711153\n",
      "Loss = 2.3343295721297137\n",
      "Loss = 2.3704731428175707\n",
      "Loss = 2.358194741179187\n",
      "Loss = 2.3622655392001803\n",
      "Loss = 2.333208948866811\n",
      "Loss = 2.3161095114719914\n",
      "Loss = 2.334560001508302\n",
      "Loss = 2.3756406731363136\n",
      "Loss = 2.318185361605474\n",
      "Loss = 2.3159646493923827\n",
      "Loss = 2.3335161111300122\n",
      "Loss = 2.3668085090574866\n",
      "Loss = 2.357255195992775\n",
      "Loss = 2.286602696001082\n",
      "Loss = 2.3286811011258397\n",
      "Loss = 2.33854832214968\n",
      "Loss = 2.3402289376983205\n",
      "Loss = 2.3450558536889967\n",
      "Loss = 2.359643071246229\n",
      "Loss = 2.3234916535438193\n",
      "Loss = 2.3335287292937386\n",
      "Loss = 2.297173140905172\n",
      "Loss = 2.3310919069244864\n",
      "Loss = 2.3229973554915953\n",
      "Loss = 2.306214218780418\n",
      "Loss = 2.3376904779337115\n",
      "Loss = 2.35933708371376\n",
      "Loss = 2.336279323823213\n",
      "Loss = 2.32392791458701\n",
      "Loss = 2.3329252427669482\n",
      "Layer 1 mean: -0.0014072252876741206 std: 0.4270314157418254\n",
      "Layer 2 mean: 0.013280827664406742 std: 0.3715401606352546\n",
      "Layer 3 mean: -0.14805259333432738 std: 0.23612726447432617\n",
      "Loss = 2.3275893996868184\n",
      "Loss = 2.3290003438420803\n",
      "Loss = 2.3281719548570674\n",
      "Loss = 2.2964636699337357\n",
      "Loss = 2.3399749156044054\n",
      "Loss = 2.3373826508017257\n",
      "Loss = 2.315580570377806\n",
      "Loss = 2.3116390677009435\n",
      "Loss = 2.3440781651188694\n",
      "Loss = 2.3339289312601927\n",
      "Loss = 2.3164782666902664\n",
      "Loss = 2.3092067325191206\n",
      "Loss = 2.361961042862445\n",
      "Loss = 2.3341740891537537\n",
      "Loss = 2.3003074405312365\n",
      "Loss = 2.3122502345456617\n",
      "Loss = 2.3329882963317745\n",
      "Loss = 2.3124688565263085\n",
      "Loss = 2.3090596940246413\n",
      "Loss = 2.3448882611132964\n",
      "Loss = 2.339029972311583\n",
      "Loss = 2.2860424864989657\n",
      "Loss = 2.3457462654888617\n",
      "Loss = 2.348190560057154\n",
      "Loss = 2.3486641794862626\n",
      "Loss = 2.325383129643537\n",
      "Loss = 2.34613993539107\n",
      "Loss = 2.3137407498384213\n",
      "Loss = 2.338064375382352\n",
      "Loss = 2.370003914087971\n",
      "Loss = 2.349283882979467\n",
      "Loss = 2.328566716500932\n",
      "Loss = 2.268254813682349\n",
      "Loss = 2.3451876802253935\n",
      "Loss = 2.283633969230232\n",
      "Loss = 2.3171367168276698\n",
      "Loss = 2.3558116933490503\n",
      "Loss = 2.356916176047794\n",
      "Loss = 2.3471562159525607\n",
      "Loss = 2.3452313870188775\n",
      "Loss = 2.3452415159952436\n",
      "Loss = 2.3294318853970486\n",
      "Loss = 2.3200312367862574\n",
      "Loss = 2.3176614859418807\n",
      "Loss = 2.349019522107233\n",
      "Loss = 2.3337528524708215\n",
      "Loss = 2.339227005451905\n",
      "Loss = 2.315614884832608\n",
      "Loss = 2.3508478874423395\n",
      "Loss = 2.3672738959402437\n",
      "Loss = 2.331638262608407\n",
      "Loss = 2.329515817831159\n",
      "Loss = 2.3173778214675878\n",
      "Loss = 2.3117497941530685\n",
      "Loss = 2.329844768780792\n",
      "Loss = 2.3250910140350287\n",
      "Loss = 2.327713021036062\n",
      "Loss = 2.3305579572379376\n",
      "Loss = 2.33296629371032\n",
      "Loss = 2.3271120679967785\n",
      "Loss = 2.287215371233953\n",
      "Loss = 2.373374474717875\n",
      "Loss = 2.3239593620061574\n",
      "Loss = 2.3093570222685025\n",
      "Loss = 2.306334000671187\n",
      "Loss = 2.3259409665525688\n",
      "Loss = 2.334580249137119\n",
      "Loss = 2.308427207837508\n",
      "Loss = 2.3351349308490983\n",
      "Loss = 2.320102454325532\n",
      "Loss = 2.306063162272601\n",
      "Loss = 2.342973070465771\n",
      "Loss = 2.2941997118330004\n",
      "Loss = 2.322902925273768\n",
      "Loss = 2.332278779421523\n",
      "Loss = 2.320233567984406\n",
      "Loss = 2.3269860479684583\n",
      "Loss = 2.3147777857184604\n",
      "Loss = 2.3482679301543534\n",
      "Loss = 2.3346875023460765\n",
      "Loss = 2.361286916605663\n",
      "Loss = 2.320328567512609\n",
      "Loss = 2.329462401870824\n",
      "Loss = 2.348689365136212\n",
      "Loss = 2.312007945504576\n",
      "Loss = 2.31659281559497\n",
      "Loss = 2.32183610526433\n",
      "Loss = 2.2939310487902294\n",
      "Loss = 2.3207760976198695\n",
      "Loss = 2.3144281760904377\n",
      "Loss = 2.330361842310732\n",
      "Loss = 2.326787512708458\n",
      "Loss = 2.3139503769065977\n",
      "Loss = 2.3367070901592877\n",
      "Loss = 2.3324686899184712\n",
      "Loss = 2.3198613574587785\n",
      "Loss = 2.282944949103772\n",
      "Loss = 2.314798182420648\n",
      "Loss = 2.318204352589879\n",
      "Loss = 2.288546066708072\n",
      "Layer 1 mean: 0.005319878041748263 std: 0.42912075569622565\n",
      "Layer 2 mean: 0.01696657354618536 std: 0.37360109172735656\n",
      "Layer 3 mean: -0.1403601522823306 std: 0.23847635181847096\n",
      "Loss = 2.2888610888601164\n",
      "Loss = 2.347204985649161\n",
      "Loss = 2.3258471297019057\n",
      "Loss = 2.332687387799637\n",
      "Loss = 2.3342067209748105\n",
      "Loss = 2.343083581218001\n",
      "Loss = 2.3349374238418275\n",
      "Loss = 2.3092804948430103\n",
      "Loss = 2.3219013889666456\n",
      "Loss = 2.3059347208652308\n",
      "Loss = 2.322522318502857\n",
      "Loss = 2.329883374009291\n",
      "Loss = 2.3165188333876947\n",
      "Loss = 2.3009223356105886\n",
      "Loss = 2.2995854801339117\n",
      "Loss = 2.3070648047382187\n",
      "Loss = 2.2813284097914996\n",
      "Loss = 2.32549011343682\n",
      "Loss = 2.322056766243656\n",
      "Loss = 2.33616877795833\n",
      "Loss = 2.333938538275161\n",
      "Loss = 2.308386183640426\n",
      "Loss = 2.311959763334115\n",
      "Loss = 2.3214488109247977\n",
      "Loss = 2.313599945117488\n",
      "Loss = 2.330984936722235\n",
      "Loss = 2.3146889996033577\n",
      "Loss = 2.3461326852754443\n",
      "Loss = 2.3222235312993984\n",
      "Loss = 2.343364541959893\n",
      "Loss = 2.333884871558131\n",
      "Loss = 2.3317814004294872\n",
      "Loss = 2.3301978941243977\n",
      "Loss = 2.3525180427322727\n",
      "Loss = 2.3399314701723886\n",
      "Loss = 2.321084694027535\n",
      "Loss = 2.294433117596368\n",
      "Loss = 2.3150513690805816\n",
      "Loss = 2.350739531140295\n",
      "Loss = 2.3274912266874765\n",
      "Loss = 2.3092815952922603\n",
      "Loss = 2.3377037099771325\n",
      "Loss = 2.3076615739387254\n",
      "Loss = 2.3495570602625495\n",
      "Loss = 2.288888163673527\n",
      "Loss = 2.3184298591179573\n",
      "Loss = 2.3240399735504482\n",
      "Loss = 2.314229948137821\n",
      "Loss = 2.3182940752923518\n",
      "Loss = 2.3317205920971054\n",
      "Loss = 2.3375066381829166\n",
      "Loss = 2.332371354013753\n",
      "Loss = 2.322130671817969\n",
      "Loss = 2.315639219465178\n",
      "Loss = 2.3151982414792145\n",
      "Loss = 2.3199762877059125\n",
      "Loss = 2.3133887962851194\n",
      "Loss = 2.3292309411361836\n",
      "Loss = 2.333882985749137\n",
      "Loss = 2.3214102040819067\n",
      "Loss = 2.305134992569911\n",
      "Loss = 2.321364747909249\n",
      "Loss = 2.3399365370115244\n",
      "Loss = 2.3452736120178748\n",
      "Loss = 2.3103204018173065\n",
      "Loss = 2.3111741760265434\n",
      "Loss = 2.2896343187264434\n",
      "Loss = 2.3101797787085996\n",
      "Loss = 2.3085506231938115\n",
      "Loss = 2.284350495508903\n",
      "Loss = 2.310170440737494\n",
      "Loss = 2.3165706303454607\n",
      "Loss = 2.3089720926997788\n",
      "Loss = 2.279623976882291\n",
      "Loss = 2.3179345876825592\n",
      "Loss = 2.2937485934636603\n",
      "Loss = 2.3223555682464925\n",
      "Loss = 2.307337388016575\n",
      "Loss = 2.3262386398732025\n",
      "Loss = 2.309637694069185\n",
      "Loss = 2.3365651040378665\n",
      "Loss = 2.325324953514785\n",
      "Loss = 2.276553129683525\n",
      "Loss = 2.3101025836143085\n",
      "Loss = 2.3169780867396286\n",
      "Loss = 2.330072163508027\n",
      "Loss = 2.3342939960221543\n",
      "Loss = 2.3349641672683594\n",
      "Loss = 2.2895234549838133\n",
      "Loss = 2.3444676278617997\n",
      "Loss = 2.3350832096695058\n",
      "Loss = 2.337877699737084\n",
      "Loss = 2.3150820911492245\n",
      "Loss = 2.312837911323757\n",
      "Loss = 2.3276377365742564\n",
      "Loss = 2.3335171184245413\n",
      "Loss = 2.3188314801619345\n",
      "Loss = 2.2938858909454662\n",
      "Loss = 2.320525584243218\n",
      "Loss = 2.312072422037608\n",
      "Layer 1 mean: 0.006109143045349231 std: 0.44149425735570524\n",
      "Layer 2 mean: 0.02034590042956273 std: 0.38858420703979846\n",
      "Layer 3 mean: -0.14177223438246978 std: 0.22914579587897937\n",
      "Loss = 2.313902332428114\n",
      "Loss = 2.3052352316738425\n",
      "Loss = 2.299601016312144\n",
      "Loss = 2.3516050289798676\n",
      "Loss = 2.312282849551936\n",
      "Loss = 2.329177516883317\n",
      "Loss = 2.3147147247972177\n",
      "Loss = 2.323801348107291\n",
      "Loss = 2.317196179164505\n",
      "Loss = 2.325822234443078\n",
      "Loss = 2.3504094175078256\n",
      "Loss = 2.3521449849905616\n",
      "Loss = 2.2969205620270667\n",
      "Loss = 2.2975229179943435\n",
      "Loss = 2.3236101753544984\n",
      "Loss = 2.3256111109473077\n",
      "Loss = 2.3241880608314345\n",
      "Loss = 2.3042286274385324\n",
      "Loss = 2.2925195526836197\n",
      "Loss = 2.3278445532163885\n",
      "Loss = 2.3230135160352066\n",
      "Loss = 2.330295581465115\n",
      "Loss = 2.3606010102669424\n",
      "Loss = 2.3197147112640533\n",
      "Loss = 2.302482710456788\n",
      "Loss = 2.308639356168647\n",
      "Loss = 2.360085101381549\n",
      "Loss = 2.3161983959559196\n",
      "Loss = 2.3009528794265965\n",
      "Loss = 2.3247498954858106\n",
      "Loss = 2.3151175538957256\n",
      "Loss = 2.3022975121714344\n",
      "Loss = 2.302050841989243\n",
      "Loss = 2.3234976212187735\n",
      "Loss = 2.337001834026297\n",
      "Loss = 2.3225890983492676\n",
      "Loss = 2.337262875192641\n",
      "Loss = 2.327859676947296\n",
      "Loss = 2.313286417177209\n",
      "Loss = 2.327779404381496\n",
      "Loss = 2.299332945968854\n",
      "Loss = 2.329311110816273\n",
      "Loss = 2.291139943299363\n",
      "Loss = 2.3003739700375867\n",
      "Loss = 2.318376883269952\n",
      "Loss = 2.3404601043481392\n",
      "Loss = 2.3566674542403945\n",
      "Loss = 2.3267578822746384\n",
      "Loss = 2.275473428792046\n",
      "Loss = 2.336965554807241\n",
      "Loss = 2.300657883353358\n",
      "Loss = 2.3306598844434747\n",
      "Loss = 2.3125435621954193\n",
      "Loss = 2.3175207391010764\n",
      "Loss = 2.327769559252059\n",
      "Loss = 2.3359724773610258\n",
      "Loss = 2.3203196294414914\n",
      "Loss = 2.312827149774407\n",
      "Loss = 2.2917459427496523\n",
      "Loss = 2.3537561598685013\n",
      "Loss = 2.3142202189340013\n",
      "Loss = 2.2910691860195778\n",
      "Loss = 2.310866876946669\n",
      "Loss = 2.311948359174634\n",
      "Loss = 2.3407873246520636\n",
      "Loss = 2.3383935113862364\n",
      "Loss = 2.325717678819476\n",
      "Loss = 2.3073206541246907\n",
      "Loss = 2.3109033955837104\n",
      "Loss = 2.3158254567374064\n",
      "Loss = 2.3238312970707664\n",
      "Loss = 2.3211006569964523\n",
      "Loss = 2.3288193972824205\n",
      "Loss = 2.327944309696436\n",
      "Loss = 2.326888149308332\n",
      "Loss = 2.2939058395797103\n",
      "Loss = 2.335973945822489\n",
      "Loss = 2.341644018470304\n",
      "Loss = 2.3268987595976247\n",
      "Loss = 2.3135360285941395\n",
      "Loss = 2.3157789105782935\n",
      "Loss = 2.321454660556348\n",
      "Loss = 2.3303565440747933\n",
      "Loss = 2.3263078705725224\n",
      "Loss = 2.3342366260078298\n",
      "Loss = 2.30003951912743\n",
      "Loss = 2.324768125291363\n",
      "Loss = 2.3195202023629173\n",
      "Loss = 2.2621887264913108\n",
      "Loss = 2.3179997780151806\n",
      "Loss = 2.321724933154777\n",
      "Loss = 2.3156676135483063\n",
      "Loss = 2.3423270126509954\n",
      "Loss = 2.345566759499162\n",
      "Loss = 2.3378266915532238\n",
      "Loss = 2.315006048376609\n",
      "Loss = 2.314898230674205\n",
      "Loss = 2.3024372224276615\n",
      "Loss = 2.310686081937284\n",
      "Loss = 2.311504972665113\n",
      "Layer 1 mean: 0.004036685000943559 std: 0.4418549621287943\n",
      "Layer 2 mean: 0.016490543811750406 std: 0.39430278445591005\n",
      "Layer 3 mean: -0.16782765081657064 std: 0.2176638925003605\n",
      "Loss = 2.3281798880014777\n",
      "Loss = 2.324744122234731\n",
      "Loss = 2.3218678535569226\n",
      "Loss = 2.317549521243091\n",
      "Loss = 2.3070348240374408\n",
      "Loss = 2.3135991952052115\n",
      "Loss = 2.3101197198812984\n",
      "Loss = 2.3137314659702275\n",
      "Loss = 2.3262311950398935\n",
      "Loss = 2.2978082658761982\n",
      "Loss = 2.3120943398344247\n",
      "Loss = 2.319495590456134\n",
      "Loss = 2.3075646858393606\n",
      "Loss = 2.324183556797486\n",
      "Loss = 2.310465693286881\n",
      "Loss = 2.347233538758872\n",
      "Loss = 2.3366198771260764\n",
      "Loss = 2.342107096739312\n",
      "Loss = 2.3155060937802268\n",
      "Loss = 2.275940649213359\n",
      "Loss = 2.323303260416843\n",
      "Loss = 2.2856169706665557\n",
      "Loss = 2.3007724351792582\n",
      "Loss = 2.3098028937744806\n",
      "Loss = 2.3260509858786764\n",
      "Loss = 2.2999127899877725\n",
      "Loss = 2.307467574730172\n",
      "Loss = 2.32456199028665\n",
      "Loss = 2.3049886514415485\n",
      "Loss = 2.264943914884996\n",
      "Loss = 2.319174435014839\n",
      "Loss = 2.3142653635323303\n",
      "Loss = 2.3218670507815\n",
      "Loss = 2.3250409767461093\n",
      "Loss = 2.2978356638721316\n",
      "Loss = 2.3410148430051354\n",
      "Loss = 2.3191116645159786\n",
      "Loss = 2.285389062962177\n",
      "Loss = 2.2932662628573226\n",
      "Loss = 2.331243586938424\n",
      "Loss = 2.336076207682391\n",
      "Loss = 2.2926310042542264\n",
      "Loss = 2.2961426916927588\n",
      "Loss = 2.308654953899294\n",
      "Loss = 2.3305023016616517\n",
      "Loss = 2.306197357069048\n",
      "Loss = 2.314456842789155\n",
      "Loss = 2.3027150597560375\n",
      "Loss = 2.3033826388672347\n",
      "Loss = 2.338125051901792\n",
      "Loss = 2.32155814083072\n",
      "Loss = 2.3109471617973156\n",
      "Loss = 2.332638772823321\n",
      "Loss = 2.34080589854081\n",
      "Loss = 2.3000332107223773\n",
      "Loss = 2.3256126085209248\n",
      "Loss = 2.3203787147314845\n",
      "Loss = 2.309316139221555\n",
      "Loss = 2.3222506924731254\n",
      "Loss = 2.2993509335880673\n",
      "Loss = 2.3269222469877144\n",
      "Loss = 2.319649403342228\n",
      "Loss = 2.3127126434638594\n",
      "Loss = 2.322553365472743\n",
      "Loss = 2.3158107468724007\n",
      "Loss = 2.323639519204248\n",
      "Loss = 2.308415411527748\n",
      "Loss = 2.358214003483318\n",
      "Loss = 2.3007448876599774\n",
      "Loss = 2.295703168122512\n",
      "Loss = 2.3166369786032903\n",
      "Loss = 2.301623652481588\n",
      "Loss = 2.30265752470201\n",
      "Loss = 2.319580965935003\n",
      "Loss = 2.3105995389450995\n",
      "Loss = 2.3241899863137463\n",
      "Loss = 2.3064459866595266\n",
      "Loss = 2.3149104553138393\n",
      "Loss = 2.3115960220427514\n",
      "Loss = 2.3096743170845517\n",
      "Loss = 2.3111641460956465\n",
      "Loss = 2.2914969522313684\n",
      "Loss = 2.3030939075874937\n",
      "Loss = 2.335023112801113\n",
      "Loss = 2.300767288633512\n",
      "Loss = 2.328476857995191\n",
      "Loss = 2.315968389506091\n",
      "Loss = 2.3257134086800604\n",
      "Loss = 2.307289286350181\n",
      "Loss = 2.3225168163361065\n",
      "Loss = 2.3162709816172375\n",
      "Loss = 2.311372418085189\n",
      "Loss = 2.3196519876751913\n",
      "Loss = 2.3168005472928024\n",
      "Loss = 2.2855531950014143\n",
      "Loss = 2.3083799608886553\n",
      "Loss = 2.31041926279615\n",
      "Loss = 2.3333523727100216\n",
      "Loss = 2.296232433107693\n",
      "Loss = 2.295129342975989\n",
      "Layer 1 mean: 0.0065661730430912725 std: 0.46154537676845836\n",
      "Layer 2 mean: 0.016200271685191444 std: 0.40714441231545623\n",
      "Layer 3 mean: -0.16227225289286928 std: 0.21081950392245807\n",
      "Loss = 2.2975086162946448\n",
      "Loss = 2.2934799804599555\n",
      "Loss = 2.299989177058683\n",
      "Loss = 2.2774443793797805\n",
      "Loss = 2.316337794886489\n",
      "Loss = 2.317865631541779\n",
      "Loss = 2.322090237941545\n",
      "Loss = 2.303017300423399\n",
      "Loss = 2.2950148841928364\n",
      "Loss = 2.28723705966099\n",
      "Loss = 2.3267284985958594\n",
      "Loss = 2.316398069546069\n",
      "Loss = 2.304557521212699\n",
      "Loss = 2.2989560003467124\n",
      "Loss = 2.292308556890396\n",
      "Loss = 2.2971726423948193\n",
      "Loss = 2.3014138079689923\n",
      "Loss = 2.3253822410713947\n",
      "Loss = 2.3351510913500615\n",
      "Loss = 2.312742028451612\n",
      "Loss = 2.335578580262675\n",
      "Loss = 2.295010409645402\n",
      "Loss = 2.298312019483273\n",
      "Loss = 2.2893353956028997\n",
      "Loss = 2.309659673774264\n",
      "Loss = 2.294628240477243\n",
      "Loss = 2.281526734835916\n",
      "Loss = 2.3280297911710948\n",
      "Loss = 2.324282374735405\n",
      "Loss = 2.313722353118056\n",
      "Loss = 2.2934269311571627\n",
      "Loss = 2.3102421108334767\n",
      "Loss = 2.3212868586000095\n",
      "Loss = 2.3001505715082944\n",
      "Loss = 2.316896357432646\n",
      "Loss = 2.334728677401796\n",
      "Loss = 2.313023055789983\n",
      "Loss = 2.3267197593901696\n",
      "Loss = 2.301195951894231\n",
      "Loss = 2.30910679852886\n",
      "Loss = 2.3294052615244065\n",
      "Loss = 2.3148026908666095\n",
      "Loss = 2.2921553693872734\n",
      "Loss = 2.290964365242217\n",
      "Loss = 2.305859051895177\n",
      "Loss = 2.3093806310500335\n",
      "Loss = 2.3297081407020364\n",
      "Loss = 2.303877350733818\n",
      "Loss = 2.299635715637299\n",
      "Loss = 2.3136507889928746\n",
      "Loss = 2.300221434864035\n",
      "Loss = 2.3344918129770056\n",
      "Loss = 2.324095631129139\n",
      "Loss = 2.316421421439276\n",
      "Loss = 2.3244505459539493\n",
      "Loss = 2.311208755387519\n",
      "Loss = 2.3234643875828573\n",
      "Loss = 2.316140504815686\n",
      "Loss = 2.322683959111239\n",
      "Loss = 2.3071865189097043\n",
      "Loss = 2.319310192912787\n",
      "Loss = 2.3003498392154755\n",
      "Loss = 2.3258329742868655\n",
      "Loss = 2.3172451346455087\n",
      "Loss = 2.3423545625912805\n",
      "Loss = 2.3274608102341823\n",
      "Loss = 2.3095882023325696\n",
      "Loss = 2.299834692067707\n",
      "Loss = 2.348963587856028\n",
      "Loss = 2.3017467345403437\n",
      "Loss = 2.324832522518125\n",
      "Loss = 2.296005162998193\n",
      "Loss = 2.282360395279696\n",
      "Loss = 2.341936084087678\n",
      "Loss = 2.291588026036247\n",
      "Loss = 2.296365627427316\n",
      "Loss = 2.323359538295537\n",
      "Loss = 2.289131657836955\n",
      "Loss = 2.316042830117648\n",
      "Loss = 2.3021099592909797\n",
      "Loss = 2.3103056977706196\n",
      "Loss = 2.301987752430703\n",
      "Loss = 2.3069981206889465\n",
      "Loss = 2.3119533603475535\n",
      "Loss = 2.3252816823752314\n",
      "Loss = 2.31726347748893\n",
      "Loss = 2.3203455946716827\n",
      "Loss = 2.3072037263794085\n",
      "Loss = 2.3130649514189234\n",
      "Loss = 2.325393432666666\n",
      "Loss = 2.3250888201479594\n",
      "Loss = 2.3284147997371254\n",
      "Loss = 2.3183661274772884\n",
      "Loss = 2.288873369077744\n",
      "Loss = 2.3168404188588028\n",
      "Loss = 2.2606115153749484\n",
      "Loss = 2.320953306200532\n",
      "Loss = 2.3012527127392595\n",
      "Loss = 2.3202905577939466\n",
      "Loss = 2.303693763174649\n",
      "Layer 1 mean: 0.010237649994117259 std: 0.460192004747091\n",
      "Layer 2 mean: 0.018740599192734537 std: 0.4102150425214176\n",
      "Layer 3 mean: -0.1575166947062019 std: 0.2181836596216261\n",
      "Loss = 2.338198743310378\n",
      "Loss = 2.325041199285203\n",
      "Loss = 2.334813985825242\n",
      "Loss = 2.334073239981735\n",
      "Loss = 2.2873553555878487\n",
      "Loss = 2.32827309774303\n",
      "Loss = 2.329502716640244\n",
      "Loss = 2.3287304554063346\n",
      "Loss = 2.32342425906898\n",
      "Loss = 2.292991010297539\n",
      "Loss = 2.3069777807771725\n",
      "Loss = 2.325132461087229\n",
      "Loss = 2.3216449177784666\n",
      "Loss = 2.3162610172124203\n",
      "Loss = 2.317537937497213\n",
      "Loss = 2.303954958850171\n",
      "Loss = 2.3184396619308343\n",
      "Loss = 2.3252837569411335\n",
      "Loss = 2.297356764319852\n",
      "Loss = 2.324618114949227\n",
      "Loss = 2.3260770437629805\n",
      "Loss = 2.2896393498823615\n",
      "Loss = 2.318933401587027\n",
      "Loss = 2.299343577072539\n",
      "Loss = 2.2770950119097217\n",
      "Loss = 2.2911943802228705\n",
      "Loss = 2.296115067030704\n",
      "Loss = 2.326009831313123\n",
      "Loss = 2.336485264958128\n",
      "Loss = 2.3264925306660134\n",
      "Loss = 2.293932100500216\n",
      "Loss = 2.3174585267333514\n",
      "Loss = 2.321352432497113\n",
      "Loss = 2.30411763132558\n",
      "Loss = 2.304044752416918\n",
      "Loss = 2.3319890358368327\n",
      "Loss = 2.345283471279772\n",
      "Loss = 2.327515584999273\n",
      "Loss = 2.320027068496767\n",
      "Loss = 2.315544161730168\n",
      "Loss = 2.2977897855021765\n",
      "Loss = 2.3052378995697107\n",
      "Loss = 2.3185121918981535\n",
      "Loss = 2.3169766096529285\n",
      "Loss = 2.316004951885972\n",
      "Loss = 2.313847541315692\n",
      "Loss = 2.3246315362775\n",
      "Loss = 2.309351373746417\n",
      "Loss = 2.3209550806401413\n",
      "Loss = 2.3073613849600685\n",
      "Loss = 2.307406907496027\n",
      "Loss = 2.3330486430284285\n",
      "Loss = 2.310276443377078\n",
      "Loss = 2.3359643633808598\n",
      "Loss = 2.319422456032842\n",
      "Loss = 2.3324076895819257\n",
      "Loss = 2.322947777841579\n",
      "Loss = 2.3310527241940746\n",
      "Loss = 2.3271909927442467\n",
      "Loss = 2.3306352703316136\n",
      "Loss = 2.3024965545055673\n",
      "Loss = 2.331330562355234\n",
      "Loss = 2.33129774213415\n",
      "Loss = 2.288170239224826\n",
      "Loss = 2.330143128806103\n",
      "Loss = 2.292271428484944\n",
      "Loss = 2.29582623223667\n",
      "Loss = 2.317994340177873\n",
      "Loss = 2.3117600741037734\n",
      "Loss = 2.3138821540023597\n",
      "Loss = 2.304563243836783\n",
      "Loss = 2.3041473064281783\n",
      "Loss = 2.3190464517826226\n",
      "Loss = 2.3105018620217392\n",
      "Loss = 2.3217881170641888\n",
      "Loss = 2.297509023967817\n",
      "Loss = 2.3197131147436227\n",
      "Loss = 2.31238212574654\n",
      "Loss = 2.3159915691791015\n",
      "Loss = 2.2917222825782613\n",
      "Loss = 2.318154730482113\n",
      "Loss = 2.2932502691561623\n",
      "Loss = 2.316017063661911\n",
      "Loss = 2.2900051311763914\n",
      "Loss = 2.3142346023259597\n",
      "Loss = 2.2952253980728625\n",
      "Loss = 2.3094703737891615\n",
      "Loss = 2.305927720519497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[384]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m dtanhlayer = dlayer2 @ W2.T \u001b[38;5;66;03m#layer2 = tanhlayer @ W2 + b2\u001b[39;00m\n\u001b[32m     39\u001b[39m dW2 = tanhlayer.T @ dlayer2  \n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m db2 = \u001b[43mdlayer2\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m     41\u001b[39m dlayer1 = (\u001b[32m1\u001b[39m-tanhlayer** \u001b[32m2\u001b[39m) * dtanhlayer \u001b[38;5;66;03m#tanhlayer = np.tanh(layer1)\u001b[39;00m\n\u001b[32m     42\u001b[39m dW1 = mini_batch.T @ dlayer1  \u001b[38;5;66;03m#layer1 = mini_batch @ W1 + b1\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\_methods.py:50\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     47\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     51\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     55\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    #Mini-Batching\n",
    "    batch_size = 128\n",
    "    epsilon = 1e-9\n",
    "    ind = np.random.randint(low=0,high=Xtr.shape[0],size=(batch_size,))\n",
    "    mini_batch = Xtr[ind]\n",
    "    if i == 0:\n",
    "        print(f'Mini batch mean: {np.mean(mini_batch)} std: {np.std(mini_batch)}')\n",
    "\n",
    "    #Forward pass\n",
    "    layer1 = mini_batch @ W1 + b1\n",
    "    if i % 100 == 0:\n",
    "        print(f'Layer 1 mean: {np.mean(layer1)} std: {np.std(layer1)}')\n",
    "    tanhlayer = np.tanh(layer1)\n",
    "    layer2 = tanhlayer @ W2 + b2\n",
    "    if i % 100 == 0:\n",
    "        print(f'Layer 2 mean: {np.mean(layer2)} std: {np.std(layer2)}')\n",
    "    tanhlayer2 = np.tanh(layer2)\n",
    "    layer3 = tanhlayer2 @ W3 + b3\n",
    "    if i % 100 == 0:\n",
    "        print(f'Layer 3 mean: {np.mean(layer3)} std: {np.std(layer3)}')\n",
    "    softmax = np.exp(layer3) / np.sum(np.exp(layer3), axis = 1, keepdims=True)\n",
    "    one_hot = np.zeros((batch_size,10))\n",
    "    for k in range(batch_size):\n",
    "        one_hot[k,Ytr[ind][k]] += 1\n",
    "    cross_entropy_loss = 0\n",
    "    correct_class_labels = Ytr[ind]\n",
    "    correct_logprobs = -np.log(softmax[range(batch_size), correct_class_labels])\n",
    "    cross_entropy_loss = np.mean(correct_logprobs)\n",
    "    print(f'Loss = {cross_entropy_loss}')\n",
    "\n",
    "    #Backward pass\n",
    "    dlayer3 = (softmax - one_hot) / batch_size\n",
    "    dtanhlayer2 = dlayer3 @ W3.T #layer3 = tanhlayer2 @ W3 + b3\n",
    "    dW3 = tanhlayer2.T @ dlayer3\n",
    "    db3 = dlayer3.sum(0)  \n",
    "    dlayer2 = (1-tanhlayer2**2) * dtanhlayer2 #tanhlayer2 = np.tanh(layer2)\n",
    "    dtanhlayer = dlayer2 @ W2.T #layer2 = tanhlayer @ W2 + b2\n",
    "    dW2 = tanhlayer.T @ dlayer2  \n",
    "    db2 = dlayer2.sum(0)  \n",
    "    dlayer1 = (1-tanhlayer** 2) * dtanhlayer #tanhlayer = np.tanh(layer1)\n",
    "    dW1 = mini_batch.T @ dlayer1  #layer1 = mini_batch @ W1 + b1\n",
    "    db1 = dlayer1.sum(0)\n",
    "    gradient_params = [dW1,db1,dW2,db2,dW3,db3]\n",
    "\n",
    "    lr = 1e-2\n",
    "    for j in range(len(parameters)):\n",
    "        parameters[j] = parameters[j] - lr * gradient_params[j]\n",
    "    W1,b1,W2,b2,W3,b3 = tuple(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
